---
title: "Dengue Forecasting Project"
author: "Raghvendra Jain"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    fig_caption: yes
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#Abstract

To create early warning system of dengue outbreaks, we present a machine learning-based methodology capable of providing real-time (“nowcast”) and forecast estimates of dengue prediction in each of the fifty districts of Thailand by leveraging data from multiple data sources. Using a set of prediction variables we show an increasing prediction accuracy of the model with an optimal combination of predictors which include: meteorological data, clinical data, lag variables of disease surveillance, socio-economic data and the data  encoding spatial dependence on dengue transmission. We use generalized Generalized Additive Models (GAMs) to fit the relationships between the predictors and the clinical data of Dengue hemorrhagic fever (DHF) on the basis of the data from 2008 to 2012. Using the data from  2013 to 2015 and a comparative set of prediction models we evaluate the predictive ability of the fitted models according to RMSE and SRMSE, BIC as well as AIC. We also show that for the prediction of dengue outbreaks within a district, the influence of dengue incidences and socio-economic data from the surrounding districts is **statistically significant**, possibly indicating the influence of movement patterns of people and spatial heterogeneity of human activities on the spread of the epidemic.

# Hypothesis

$H_1:$ To forecast dengue incidences in a particular district, the influence of the data from past dengue incidences and socio-economic data from its **surrounding districts** is statistically significant.  

$H_2:$ To forecast dengue incidences, a data-driven interpretable, non-parametric time-series forecasting approach (e.g. Generalized Additive Models (GAMs)) is statistically better than parametric modeling approaches (e.g. ARIMA.)

$H_3:$ To forecast dengue incidences, an ensemble forecasting model with Bayesian Network and time-series modeling approach is statistically better than the individual models. 




# Exploratory Data Analysis

I import the important the following packages into R. 

```{r LoadPackages, echo=TRUE, message=FALSE}

require(ggplot2) # used in plotting 
require(reshape)
require(plotly)
require(bnlearn) # used for Bayesian Networks
require(mgcv)    # used for Generalized Additive Modeling 
require(fpp)
require(lubridate)
require(bsts)
require(dplyr)
require(CausalImpact)
require(xtable)
library(dplyr)
library(tidyr)

```






## The description of variables


```{r ConnectedDistrict, echo=FALSE, results='hide' }
connected_district <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/connected_district.csv")
connected_district <- connected_district[,-2] # I removed the Thai name
head(connected_district)
# print(head(connected_district), type = 'html')

```


```{r ConnectedDistrictCode, echo=FALSE, results='hide' }

connected_district_code <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/connected_district_code.csv", header = TRUE, check.names = FALSE)
head(connected_district_code)


```

```{r DistrictCode, echo=FALSE, results='hide'}

#Has geocodes, postal code of each of the district, their names, population and area.

district_code <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/district_code.csv")
head(district_code)

```


```{r CommunityData, echo=FALSE, results='hide'}

# Tells about the number of communities, population from 2014, families and households. The family and household data are same. 

district_code_with_comunity_data <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/district_code_with_comunity_data.csv")
head(district_code_with_comunity_data)

```

```{r GarbageData, echo=FALSE, results='hide'}

district_garbage_data <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/district_garbage_data.csv")
head(district_garbage_data)

```

```{r DistrictCodes, echo=FALSE, results='hide'}
match_district_code_table <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/match_district_code_table.csv")
head(match_district_code_table)
```

```{r DistrictPopulation, echo=FALSE, results='hide'}

## Result: It seems that district_population is most comprehensive. 

district_population <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/district_population.csv")
head(district_population)
```





The description of the data variables.

Variable Name | Description
------------- | -------------
connected_district_code | The rows have the codes of 50 districts of Bangkok. The colums tell its connecions to other districts.
district_code | Has geocodes, postal code of each of the district, their names, population and area.
district_garbage_data | Has yearly and daily average gardbage collection data for 3 consecutive years.
district_population | Has population deographics into age groups in each district, their total population, number of communities and the total area.




## The diurnal temperature range (DTR)

DTR is the difference between the daily maximum and minimum temperature. 

```{r DTR, echo=FALSE,  results='hide'}

AverageDTRInBangkok_2008.2015 <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/AverageDTRInBangkok_2008-2015.csv",check.names = FALSE)

AverageDTRInBangkok_2008.2015 <- subset(AverageDTRInBangkok_2008.2015, select = -2) # we remove the data of 2016
AverageDTRInBangkok_2008.2015[,"Month"] <- month.abb
AverageDTRInBangkok_2008.2015
summary_DTR <- subset(AverageDTRInBangkok_2008.2015, select = -c(Month))
```

The plot summary and visualization is given below: 

```{r DTRSummary, echo=TRUE}
AverageDTRInBangkok_2008.2015
plot.ts(summary_DTR)
```

Plotting DTR for all the years on the same plot. 

```{r DTRTogether, echo=FALSE}
meltdf <- melt(AverageDTRInBangkok_2008.2015, id.vars = "Month")

# Everything on the same plot

meltdf$Month <- factor(meltdf$Month, levels = AverageDTRInBangkok_2008.2015$Month)
ggplot(meltdf,aes(x=Month,y=value,colour=variable,group=variable)) + geom_line(size=1.5) + ggtitle("The diurnal temperature range (DTR) in Bangkok (2008-2015)") + labs(x="Months",y="DTR") 

```


##The average monthly rainfall in Bangkok.  

```{r Rain, echo=FALSE,  results='hide'}

AverageRainInBangkok_2008.2015 <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/AverageRainInBangkok_2008-2015.csv", check.names = FALSE)

AverageRainInBangkok_2008.2015 <- subset(AverageRainInBangkok_2008.2015, select = -2) #removing 2016
AverageRainInBangkok_2008.2015[,"Month"] <- month.abb
AverageRainInBangkok_2008.2015
summary_Rain<- subset(AverageRainInBangkok_2008.2015, select = -c(Month))
```

The plot summary for monthly rainfall and visualization is given below: 

```{r RainSummary, echo=TRUE}
AverageRainInBangkok_2008.2015
plot.ts(summary_Rain)
```

Plotting average monthly rainfall for all the years on the same plot. 

```{r RainTogether, echo=FALSE}
meltdf <- melt(AverageRainInBangkok_2008.2015, id.vars = "Month")
# Everything on the same plot
meltdf$Month <- factor(meltdf$Month, levels = AverageRainInBangkok_2008.2015$Month)
ggplot(meltdf,aes(x=Month,y=value,colour=variable,group=variable)) + geom_line(size=1.5) + ggtitle("Average Monthly Rainfall in Bangkok (2008-2015)") + labs(x="Months",y="Average Monthly Rainfall(mm)") 

```



## The dengue incidences in Bangkok Districts 

```{r DegueSurveillaneData, echo=FALSE, message=FALSE , results= "hide"}

## Importing the data for dengue incidences
dengue_bangkok_district_level_2008.2015 <- read.csv("E:/Dengue Forecasting Project/DataAndCode/Datasets/prepare_data/dengue_bangkok_district_level_2008-2015.csv", header = TRUE)

# Checking the data
head(dengue_bangkok_district_level_2008.2015) 

## Taking the data only for DHF. The codes are:
# DF = 66
# DHF = 26
# DSS = 27 

DHF_bangkok_district_level_2008.2015 <- subset(dengue_bangkok_district_level_2008.2015, disease_code == 26)
DHF_bangkok_district_level_2008.2015 <- subset(DHF_bangkok_district_level_2008.2015, (date_sick_year > 2007) & (date_sick_year < 2016 ))
unique(DHF_bangkok_district_level_2008.2015$date_sick_year)

# Removing the columns about disease code and district codes. The yearly data is presented below
DHF_total_2008.2015 <- subset(DHF_bangkok_district_level_2008.2015, select = - c(geocode_district, disease_code ))
head(DHF_total_2008.2015)


## Aggregating the data on DHF, according to the year. 
totalDHF <- aggregate(count ~ date_sick_year, DHF_total_2008.2015,sum)

totalDHF

ggplot(totalDHF, aes(x = date_sick_year, y = count )) + geom_bar(stat = "identity")  + ggtitle("Reported annual DHF incidents in Bangkok (2008-2015)") + labs(x="Year",y="Yearly Count Data")

```

The plot shows the Dengue hemorrhagic fever(DHF) incidence peaked in 2013 and 2015. 

```{r DegueSurveillaneData_Monthly, echo=FALSE, message=FALSE , results= "hide"}

totalDHF_month <- aggregate(count ~ date_sick_month, DHF_total_2008.2015, sum)
totalDHF_month$date_sick_month <- month.abb
totalDHF_month$date_sick_month  <- factor(totalDHF_month$date_sick_month, levels = month.abb)
ggplot(totalDHF_month, aes(x = date_sick_month, y = count )) + geom_bar(stat = "identity")  + ggtitle("Reported Monthly DHF incidents in Bangkok (2008-2015)") + labs(x="Months",y="Aggregated Monthly Count Data")

```

The above plot shows that most of the DHF incidents were reported in the month of October and November.

```{r DegueSurveillaneData_TimeWise, echo=FALSE, message=FALSE , results= "hide"}

totalDHF_TimeWise <- aggregate(count ~ date_sick_year + date_sick_month , DHF_total_2008.2015, sum)
totalDHF_TimeWise$date_sick_month <- factor(totalDHF_TimeWise$date_sick_month)
levels(totalDHF_TimeWise$date_sick_month) <- month.abb

ggplot(totalDHF_TimeWise, aes(x = date_sick_month, y = count)) + geom_bar(stat = "identity")  + ggtitle("Reported DHF incidents in Bangkok (2008-2015)") + labs(x="Months",y="Aggregated Monthly Count Data Across Years") + facet_grid(date_sick_year ~. )

```


```{r DegueSurveillaneData_DF, echo=FALSE, message=FALSE}

## Rearranging the columns 

totalDHF_TimeWise <- totalDHF_TimeWise[, c(2,1,3)]
meltdf<- melt(totalDHF_TimeWise, id.vars = c("date_sick_month", "date_sick_year"))
df_DHF <- cast(totalDHF_TimeWise, date_sick_month ~ date_sick_year )

# df_DHF <- df_DHF[c(1, 9:2)]

meltdf <- melt(df_DHF, id.vars = "date_sick_month")

ggplot(meltdf,aes(x=date_sick_month,y=value, colour = as.character(date_sick_year), group = as.character(date_sick_year) )) + geom_line(size=1.5) + ggtitle("Reported DHF incidents in Bangkok (2008-2015)") + labs(x="Months",y="Aggregated Monthly Count Data Across Years") + guides(fill=guide_legend(title=NULL))

## Reordering columns to be in the same format as that of other data frames. 
df_DHF <- df_DHF[c(1, 9:2)]

```

```{r ListOfdfs, echo= FALSE, message= FALSE, results="hide" }

## This makes the list of dataframes for each district

DHF_dist_2008.2015 <- subset(DHF_bangkok_district_level_2008.2015, select = - c(disease_code ))
length(unique(DHF_dist_2008.2015$geocode_district))

listofdfs <- list()
totalDHFdfs <- list()
meltdfs <- list()
df_DHFs <- list()

for (i in 1000:1050)
{
  listofdfs[[i]] <- subset(DHF_dist_2008.2015, geocode_district == i  )
  totalDHFdfs[[i]] <- aggregate(count ~ date_sick_year + date_sick_month, listofdfs[[i]], sum)
  meltdfs[[i]] <- melt(totalDHFdfs[[i]], id.vars = c("date_sick_month", "date_sick_year"))
  df_DHFs[[i]] <- cast( meltdfs[[i]], date_sick_month ~ date_sick_year )
  
}


```


```{r CombingingPredictors, echo= FALSE, message= FALSE, results="hide"}

## We Start with taking the large df of DHF incidents. 

head(DHF_dist_2008.2015)
names(DHF_dist_2008.2015) # "geocode_district" "date_sick_year"   "date_sick_month"  "count" 

# We add the DTR and rainfall data in the same dataframe.  
# Both these data are same for all districts. They vary only according to the year and month. 

#Ordering the dataframe according to the month.
DHF_dist_2008.2015 <- DHF_dist_2008.2015[order(DHF_dist_2008.2015$date_sick_month), ] 

#    geocode_district date_sick_year date_sick_month count
# 55              1001           2008               1     6
# 77              1001           2009               1     1
# 99              1001           2010               1    12
# 120             1001           2011               1     4
# 141             1001           2012               1     1
# 162             1001           2013               1    35

# To match with the above dataframe, I will replace the month names to integers from 1 to 12
AverageDTRInBangkok_2008.2015$Month <- c(1:12)
AverageRainInBangkok_2008.2015$Month <- c(1:12)

## Now melting the DTR and rainfall dataframes

meltDTR <- melt(AverageDTRInBangkok_2008.2015, id.vars = "Month")
names(meltDTR) <- c("date_sick_month", "date_sick_year", "DTR")

meltRainfall <- melt(AverageRainInBangkok_2008.2015, id.vars = "Month")
names(meltRainfall) <- c("date_sick_month", "date_sick_year", "Rainfall")

## Climate variables are merged together for all years and months
climateTotal <- merge(meltDTR, meltRainfall)

ggplot(climateTotal, aes(x = Rainfall, y = DTR, color )) + geom_point(size=1.5)  + ggtitle("Interaction of Climate Variables (2008-2015)") + labs(x="Rainfall (mm)",y="Diurnal Temperature Range") + facet_grid( date_sick_year ~ date_sick_month ) 

ggplot(climateTotal, aes(x = Rainfall, y = DTR, group = date_sick_year, color = date_sick_year)) + geom_point(size=1.5)  + ggtitle("Interaction of Climate Variables (2008-2015)") + labs(x="Rainfall (mm)",y="Diurnal Temperature Range") + facet_grid( . ~ date_sick_month ) 


## Merging climate variables with DHF data
total <- merge(DHF_dist_2008.2015, climateTotal) 
district_population
names(district_population)

# [1] "geocode_district"                 "district_name"                    "population_1..From.DDC.Source."  
#  [4] "Community"                        "area_km2"                         "population_2..From.Bangkok.stat."
#  [7] "Age...35"                         "Age..35"                          "X0.4"                            
# [10] "X5.9"                             "X10.14"                           "X15.19"                          
# [13] "X20.24"                           "X25.29"                           "X30.34"                          
# [16] "X35.39"                           "X40.44"                           "X45.49"                          
# [19] "X50.54"                           "X55.59"                           "X60.64"                          
# [22] "X65.69"                           "X70.74"                           "X75.79"                          
# [25] "X80.84"                           "X85.89"                           "X90.94"                          
# [28] "X95.99"                           "X..100"                          


total <- merge(total, district_population, by= "geocode_district") 
total <- merge(total, district_garbage_data, by= "geocode_district") 
any(is.na(total))

# Sum of the neighbors of each district

neighbors<- apply(connected_district_code[,-1],1, sum)
dist_neighbor <- cbind(data.frame(connected_district_code$geocode_district), data.frame(neighbors))
names(dist_neighbor) <- c("geocode_district", "neighbors")
total <- merge(total, dist_neighbor, by= "geocode_district") 

names(total)

#  [1] "geocode_district"                 "date_sick_year"                   "date_sick_month"                 
#  [4] "count"                            "DTR"                              "Rainfall"                        
#  [7] "district_name"                    "population_1..From.DDC.Source."   "Community"                       
# [10] "area_km2"                         "population_2..From.Bangkok.stat." "Age...35"                        
# [13] "Age..35"                          "X0.4"                             "X5.9"                            
# [16] "X10.14"                           "X15.19"                           "X20.24"                          
# [19] "X25.29"                           "X30.34"                           "X35.39"                          
# [22] "X40.44"                           "X45.49"                           "X50.54"                          
# [25] "X55.59"                           "X60.64"                           "X65.69"                          
# [28] "X70.74"                           "X75.79"                           "X80.84"                          
# [31] "X85.89"                           "X90.94"                           "X95.99"                          
# [34] "X..100"                           "neighbors"                       

```


```{r totalDataSaved, echo=FALSE, results="hide"}


entireData <- total 


```





```{r TimeSeries, echo= FALSE, results='hide'}
# The time series have the data for all months from 2008 to 2015:

df_DHF 
AverageDTRInBangkok_2008.2015 [, "Month"] <- month.abb
AverageRainInBangkok_2008.2015  [, "Month"] <- month.abb

```





#The Prediction Starts here 

## Using ARIMA model for The DHF Data

```{r timeSeries_DHF, echo=FALSE}

df<-df_DHF 
#Converting data frame into vectors
vec <- c(unlist(df[,9]), unlist(df[,8]), unlist(df[,7]), unlist(df[,6]), unlist(df[,5]), unlist(df[,4]), unlist(df[,3]), unlist(df[,2]))
tsa_DF<-ts(vec,start=c(2008,1),end=c(2015,12),frequency=12)

```

```{r tsPlots, echo=FALSE, results="hide"}

#This will plot the time series
plot(tsa_DF)
# This will fit in a line
abline(reg=lm(tsa_DF~time(tsa_DF)))

#This will print the cycle across years.
plot(aggregate(tsa_DF,FUN=mean))

#Box plot across months will give us a sense on seasonal effect

boxplot(tsa_DF~cycle(tsa_DF))
acf(tsa_DF)
pacf(tsa_DF)

plot.ts(tsa_DF)

fit <- stl(tsa_DF, s.window="period")
plot(fit)

# additional plots
monthplot(tsa_DF)
library(forecast)
seasonplot(tsa_DF)

```

```{r autoArima, echo=FALSE, results="hide"}


Y <- window(tsa_DF, start=c(2008, 1), end=c(2014,12))
arima <- auto.arima(Y)
### Actual versus predicted
d1 <- data.frame(c(as.numeric(fitted(arima)), # fitted and predicted
                   as.numeric(predict(arima, n.ahead = 12)$pred)),
                   as.numeric(tsa_DF), #actual values
                   as.Date(time(tsa_DF)))
names(d1) <- c("Fitted", "Actual", "Date")

### MAPE (mean absolute percentage error)
MAPE <- filter(d1, year(Date)>2014) %>% summarise(MAPE=mean(abs(Actual-Fitted)/Actual))

### Plot actual versus predicted
ggplot(data=d1, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=2) +
  theme_bw() + theme(legend.title = element_blank()) + 
  ylab("") + xlab("") +
  geom_vline(xintercept=as.numeric(as.Date("2014-12-01")), linetype=2) +
  ggtitle(paste0("ARIMA -- Holdout MAPE = ", round(100*MAPE,2), "%")) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

```

```{r ets, echo=FALSE, results="hide"}


Y <- window(tsa_DF, start=c(2008, 1), end=c(2014,12))
etsModel <- ets(Y)
### Actual versus predicted
d1 <- data.frame(c(as.numeric(fitted(etsModel)), # fitted and predicted
                   as.numeric(forecast(etsModel, h= 12)$mean)),
                   as.numeric(tsa_DF), #actual values
                   as.Date(time(tsa_DF)))
names(d1) <- c("Fitted", "Actual", "Date")

### MAPE (mean absolute percentage error)
MAPE <- filter(d1, year(Date)>2014) %>% summarise(MAPE=mean(abs(Actual-Fitted)/Actual))

### Plot actual versus predicted
ggplot(data=d1, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=2) +
  theme_bw() + theme(legend.title = element_blank()) + 
  ylab("") + xlab("") +
  geom_vline(xintercept=as.numeric(as.Date("2014-12-01")), linetype=2) +
  ggtitle(paste0("ETS -- Holdout MAPE = ", round(100*MAPE,2), "%")) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

```


##Time Series Analysis in R

First, let's plot the data

```{r plotData, echo=FALSE}

plot.ts(tsa_DF)

```

We can see from this time series that there seems to be seasonal variation in the number of dengue incidences per month: there is a peak every winter, and a trough every summer. Again, it seems that this time series could probably be described using an additive model, as the seasonal fluctuations are roughly constant in size over time and do not seem to depend on the level of the time series, and the random fluctuations also seem to be roughly constant in size over time. 

Thus, we don't need to tranform the time series by calculating the natural log of the original data. 





When a model is fit by manual setting of parameters. 


```{r ARIMA_DHF, echo=FALSE}

Y <- window(tsa_DF, start=c(2008, 1), end=c(2014,12))
arima <- arima(Y, 
               order=c(0, 1, 1), 
               seasonal=list(order=c(0,1,1), period=12))
### Actual versus predicted
d1 <- data.frame(c(as.numeric(fitted(arima)), # fitted and predicted
                   as.numeric(predict(arima, n.ahead = 12)$pred)),
                   as.numeric(tsa_DF), #actual values
                   as.Date(time(tsa_DF)))
names(d1) <- c("Fitted", "Actual", "Date")

### MAPE (mean absolute percentage error)
MAPE <- filter(d1, year(Date)>2014) %>% summarise(MAPE=mean(abs(Actual-Fitted)/Actual))

### Plot actual versus predicted
ggplot(data=d1, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=2) +
  theme_bw() + theme(legend.title = element_blank()) + 
  ylab("") + xlab("") +
  geom_vline(xintercept=as.numeric(as.Date("2014-12-01")), linetype=2) +
  ggtitle(paste0("ARIMA -- Holdout MAPE = ", round(100*MAPE,2), "%")) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

```


## A Bayesian Structural Time Series Model

```{r bsts, echo=FALSE}

Y <- window(tsa_DF, start=c(2008, 1), end=c(2014,12))
y <- Y

### Run the bsts model
ss <- AddLocalLinearTrend(list(), y)
ss <- AddSeasonal(ss, y, nseasons = 12)
bsts.model <- bsts(y, state.specification = ss, niter = 500, ping=0, seed=2016)

### Get a suggested number of burn-ins
burn <- SuggestBurn(0.1, bsts.model)

### Predict
p <- predict.bsts(bsts.model, horizon = 12, burn = burn, quantiles = c(.025, .975))

### Actual versus predicted
d2 <- data.frame(
    # fitted values and predictions
    c(as.numeric(-colMeans(bsts.model$one.step.prediction.errors[-(1:burn),])+y),  
    as.numeric(p$mean)),
    # actual data and dates 
    as.numeric(tsa_DF),
    as.Date(time(tsa_DF)))
names(d2) <- c("Fitted", "Actual", "Date")

### MAPE (mean absolute percentage error)
MAPE <- filter(d2, year(Date)>2014) %>% summarise(MAPE=mean(abs(Actual-Fitted)/Actual))

### 95% forecast credible interval
posterior.interval <- cbind.data.frame(
  as.numeric(p$interval[1,]),
  as.numeric(p$interval[2,]), 
  subset(d2, year(Date)>2014)$Date)
names(posterior.interval) <- c("LL", "UL", "Date")

### Join intervals to the forecast
d3 <- left_join(d2, posterior.interval, by="Date")

### Plot actual versus predicted with credible intervals for the holdout period
ggplot(data=d3, aes(x=Date)) +
  geom_line(aes(y=Actual, colour = "Actual"), size=1.2) +
  geom_line(aes(y=Fitted, colour = "Fitted"), size=1.2, linetype=2) +
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") +
  geom_vline(xintercept=as.numeric(as.Date("2014-12-01")), linetype=2) + 
  geom_ribbon(aes(ymin=LL, ymax=UL), fill="grey", alpha=0.5) +
  ggtitle(paste0("BSTS -- Holdout MAPE = ", round(100*MAPE,2), "%")) +
  theme(axis.text.x=element_text(angle = -90, hjust = 0))

### Extract the components
components <- cbind.data.frame(
  colMeans(bsts.model$state.contributions[-(1:burn),"trend",]),                               
  colMeans(bsts.model$state.contributions[-(1:burn),"seasonal.12.1",]),
  as.Date(time(Y)))  
names(components) <- c("Trend", "Seasonality", "Date")
components <- melt(components, id="Date")
names(components) <- c("Date", "Component", "Value")

### Plot
ggplot(data=components, aes(x=Date, y=Value)) + geom_line() + 
  theme_bw() + theme(legend.title = element_blank()) + ylab("") + xlab("") + 
  facet_grid(Component ~ ., scales="free") + guides(colour=FALSE) + 
  theme(axis.text.x=element_text(angle = -90, hjust = 0))


```

## A Structured Bayesian Network Approach

```{r HybridBayesianNet, echo=FALSE, results='hide', message=FALSE}

names(total)
bayesianDF <- subset(total, select=c("geocode_district", "date_sick_year", "date_sick_month", "count", "DTR", "Rainfall"))
bayesianDF$geocode_district <- as.factor(bayesianDF$geocode_district)
bayesianDF$date_sick_year<- as.factor(bayesianDF$date_sick_year)
bayesianDF$date_sick_month <- as.factor(bayesianDF$date_sick_month)
bayesianDF$count <- as.numeric(bayesianDF$count)
bayesianDF.train <- bayesianDF[1:4000, ]
bayesianDF.test <- bayesianDF[4001: 4719 , ]
dag = hc(bayesianDF.train)
graphviz.plot(dag)
fit = bn.fit(dag, bayesianDF.train)
pred = predict(fit, "count", bayesianDF.test)  # predicts the value of node count given test set
cbind(pred, bayesianDF.test[, "count"])         # compare the actual and predicted


```

```{r bnResults, echo= FALSE}

accuracy(f = pred, x =  bayesianDF.test[, "count"])

```

```{r newBN, echo= FALSE, results='hide'}

total<-total[order(total$date_sick_year, total$date_sick_month), ]
bayesianDF <- subset(total, select=c("geocode_district", "date_sick_year", "date_sick_month", "count", "DTR", "Rainfall"))
bayesianDF$geocode_district <- as.factor(bayesianDF$geocode_district)
bayesianDF$date_sick_year<- as.factor(bayesianDF$date_sick_year)
bayesianDF$date_sick_month <- as.factor(bayesianDF$date_sick_month)

# 

DTR <- bayesianDF$DTR
Rainfall<- bayesianDF$Rainfall
count <- bayesianDF$count
bayesianDF$count <- as.numeric(bayesianDF$count)
names(bayesianDF)[names(bayesianDF) == 'date_sick_month'] <- 'Month'
Month <- bayesianDF$Month

bayesianDF.train <- subset(bayesianDF, date_sick_year!= 2015 )
bayesianDF.train <- subset(bayesianDF.train, select=c("geocode_district", "count", "DTR", "Rainfall", "Month"))


bayesianDF.test <- subset(bayesianDF, date_sick_year== 2015 )
bayesianDF.test <- subset(bayesianDF.test, select=c("geocode_district", "count", "DTR", "Rainfall", "Month"))

dag<-model2network("[Month][geocode_district][DTR|Month][Rainfall|Month][count|DTR:Rainfall:geocode_district]")

graphviz.plot(dag)



fit_1<- bn.fit(dag, bayesianDF.train)

pred = predict(fit_1, "count", bayesianDF.test)  # predicts the value of node count given test set
cbind(pred, bayesianDF.test[, "count"])  
accuracy(f = pred, x =  bayesianDF.test[, "count"])

```



```{r}
accuracy(f = pred, x =  bayesianDF.test[, "count"])
```


## Generalized Additive Model

```{r GAM, echo=FALSE, results="hide"}
## attaching the entire dataset 


# attach(entireData)

## ordering the entire dataset according to the year and month.

orderedEntireData <-entireData[order(entireData$date_sick_year, entireData$date_sick_month), ]

orderedEntireData <- orderedEntireData %>%  group_by(geocode_district,date_sick_year, district_name.x,
                            population_1..From.DDC.Source., 
                            Community, area_km2, population_2..From.Bangkok.stat., Age...35, Age..35, X0.4, 
                            X5.9, X10.14, X15.19, X20.24,
                            X25.29, X30.34, X35.39, X40.44, X45.49, X50.54, X55.59, X60.64, X65.69, X70.74, X75.79, X80.84, X85.89, X90.94, X95.99, X..100, neighbors, 
                            Total.Gagbage.2012, Gabage.Per.day.2012,
                            Total.Gagbage.2013, Gabage.Per.day.2013, 
                            Total.Gagbage.2014, Gabage.Per.day.2014 ) %>% complete(date_sick_month = 1:12)


orderedEntireData$district_name.y <- NULL
orderedEntireData$AMP_ID <- NULL
orderedEntireData$time <- rep(1:96, 50)

orderedEntireData <- as.data.frame(orderedEntireData)

### For the ease of experimentation, lets take data of only one district
## District with code 1001 selected for experimentation

data_1001 <- subset(orderedEntireData, geocode_district == 1001)


## Creating Lag variables 
## 5 year lag for dengue data 
N <- nrow(data_1001) # taking the length of the dataframe


# putting the DHF count data into a vector so that we can tak lags
countVec <- as.vector(data_1001$count)
# Lag 0 dengue count variable
data_1001$count0 <- countVec

# to append lag variable names, I put the existing names into a column vector
temp <-colnames(data_1001)

#totally we need to take the lag of 5 years i.e. 60 months

countLag <- 1

for (i in 1:60){
  print (i);
  l <- N-i;
  countLag[i] <- paste0("count",i)
  laggedCountVec <- c(rep(NA,i), countVec[1:l]) 
  data_1001 <- cbind(data_1001, laggedCountVec)
}

# creating dengue lag variables for 60 momnths 
colnames(data_1001) <- c(temp, countLag)

## Now creating lag variables for temperature 


temperatureVec <- as.vector(data_1001$DTR)
data_1001$templ0 <- temperatureVec
temp <- colnames(data_1001)  
  
tempLag <- 1
for (i in 1:4){
  print (i);
  l <- N-i;
  tempLag[i] <- paste0("templ",i)
  laggedDTR <- c(rep(NA,i), temperatureVec[1:l]) 
  data_1001 <- cbind(data_1001, laggedDTR)
}

# creating temperature lag variables for 60 momnths 
colnames(data_1001) <- c(temp, tempLag)


## Now creating lag variables for Rainfall

# putting the Rainfall into a vector so that we can take lags
RainFallVec <- as.vector(data_1001$Rainfall)
# Lag 0 dengue count variable
data_1001$rainl0 <- RainFallVec

# to append lag variable names, I put the existing names into a column vector
temp <-colnames(data_1001)

#totally we need to take the lag of 5 years i.e. 60 months

RainLag <- 1
for (i in 1:4){
  print (i);
  l <- N-i;
  RainLag[i] <- paste0("rainl",i)
  laggedRainVec <- c(rep(NA,i), RainFallVec[1:l]) 
  data_1001 <- cbind(data_1001, laggedRainVec)
}

# creating rainfall lag variables for 60 momnths 
colnames(data_1001) <- c(temp, RainLag)


View(data_1001)

## Subset data for training 

training <- subset(data_1001, date_sick_year < 2013)



# association models
# meteorology only model - all variables all lags
mod.fmet <- gam(count ~ 
                  s(templ0, k=4) + s(templ1, k=4) + s(templ2,k=4) + s(templ3,k=4) 
                + s(rainl0,k=4) + s(rainl1,k=4) + s(rainl2,k=4) + s(rainl3,k=4)
                , family=quasipoisson, na.action=na.exclude, data = training)


summary(mod.fmet)
par(mfrow=c(1,4))
plot.gam(mod.fmet, ylim=c(-1.2,1.2), ylab="log(RR)")

# meteorology optimal model
mod.omet <- gam(training$count ~ 
                s(templ3,k=4) + s(rainl0,k=4) +  s(rainl1,k=4) +  s(rainl3,k=4), 
                family=quasipoisson, na.action=na.exclude, data = training)
summary(mod.omet)

par(mfrow=c(1,1))
p <- fitted.values(mod.omet)

plot(training$time, training$count, type="l",ylab="Number Cases",axes=F,xlab="Year")

points(predict(mod.omet, type="response"),type="l", col="red")

axis(1, at=c(6,18,30,42,54),labels=c(2008:2012))
axis(2, at=c(0,10,20,30,40,50))
title(main="Metereology Optimal")

sqrt(mean((training$count-p)^2,na.rm=T))
sqrt(mean((training$count-p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))

# AR lag  model
mod.l2 <- gam(count ~ s(count1,k=4) + s(count2,k=4) + s(count3,k=4), 
              family=quasipoisson, na.action=na.exclude, data = training)
summary(mod.l2)

par(mfrow=c(1,3))
plot.gam(mod.l2, ylim=c(-1.2,1.2), ylab="log(RR)")

## the data shows that only lag 1 is enough 

mod.l2 <- gam(count ~ s(count1,k=4), 
              family=quasipoisson, na.action=na.exclude, data = training)
summary(mod.l2)

par(mfrow=c(1,1))
plot.gam(mod.l2, ylim=c(-1.2,1.2), ylab="log(RR)")


p <- fitted.values(mod.l2)

plot(training$time, training$count, type="l")

plot(training$time, training$count, type="l",ylab="Number Cases",axes=F,xlab="Year")

points(predict(mod.l2, type="response"),type="l", col="red")

title(main="Surveillance: Short-term Lag")


axis(1, at=c(6,18,30,42,54),labels=c(2008:2012))
axis(2, at=c(0,10,20,30,40,50))
title(main="Surveillance: Short-term Lag")

sqrt(mean((training$count-p)^2,na.rm=T))
sqrt(mean((training$count-p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))




```


```{r ar, echo=FALSE, results= "hide"}

### This chunk produced no substantial results

# library(dlnm)
# 
# 
# dlag <- data.frame()
# 
# cb1.temp <- crossbasis(training$count1, lag=47, argvar=list(df=4), arglag=list(fun="poly",degree=4))
# 
# summary(cb1.temp)
# 
# lolwa <-  training$count[ !is.na( training$count ) ]
# 
# mod.dlnm <- glm(lolwa ~  cb1.temp, family=quasipoisson(), training)
# 
# 
# pred1.temp <- crosspred(cb1.temp, mod.dlnm)
# 
# par(mfrow=c(1,1))
# plot(pred1.temp, "contour")
# plot(pred1.temp)



```

```{r optimal_Model, echo=FALSE, results="hide"}

mod.fcum <- gam(count ~ s(count1,k=4) + s(count2,k=4), 
                  family=quasipoisson, na.action=na.exclude, training)
summary(mod.fcum)

par(mfrow=c(1,1))
p <- fitted.values(mod.fcum)
plot(training$time, training$count, type="l")
plot(training$time, training$count, type="l",ylab="Number Cases",axes=F,xlab="Year")
points(predict(mod.fcum, type="response"),type="l", col="red")
axis(1, at=c(6,18,30,42,54),labels=c(2008:2012))
axis(2, at=c(0,50,100,150,200,250))
title(main="Surveillance: Optimal Lag Lag")

sqrt(mean((training$count-p)^2,na.rm=T))
sqrt(mean((training$count-p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))

# final model
mod.fin <- gam(count ~  s(templ1,k=4) + s(templ2,k=4) + s(templ3,k=4)+ s(rainl0,k=4) + s(rainl1,k=4) + s(rainl3,k=4) + s(count1,k=4) , family=quasipoisson, na.action=na.exclude, data = training)

summary(mod.fin)



par(mfrow=c(1,1))
p <- fitted.values(mod.fin)
plot(training$time, training$count, type="l")
plot(training$time, training$count, type="l",ylab="Number Cases",axes=F,xlab="Year")
points(predict(mod.fin, type="response"),type="l", col="red")
axis(1, at=c(6,18,30,42,54),labels=c(2008:2012))
axis(2, at=c(0,50,100,150,200,250))
title(main="Optimal Representation B-D")

sqrt(mean((training$count-p)^2,na.rm=T))
sqrt(mean((training$count-p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))


## Taking population as offset

training$ppl <- training$population_2..From.Bangkok.stat

mod.offset <- gam(count ~  s(templ1,k=4) + s(templ2,k=4) + s(templ3,k=4)+ s(rainl0,k=4) + s(rainl1,k=4) + s(rainl3,k=4) + s(count1,k=4) + offset(log(neighbors)) , family=quasipoisson, na.action=na.exclude, data = training)

summary(mod.offset)

p <- fitted.values(mod.offset)

sqrt(mean((training$count-p)^2,na.rm=T))
sqrt(mean((training$count-p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))




```



```{r testing, echo=FALSE, results= "hide"}

mod.train <- gam(count ~  s(templ1,k=4) + s(templ2,k=4) + s(templ3,k=4)+ s(rainl0,k=4) + s(rainl1,k=4) + s(rainl3,k=4) + s(count1,k=4), family=quasipoisson, na.action=na.exclude, data = training)




summary(mod.train)

preddata <- subset(data_1001, select  =  c(templ1, templ2, templ3, rainl0, rainl1,rainl3,count1) )
data_1001$predict <-  predict(mod.train, type="response", newdata=preddata)

data_1001$p <- data_1001$predict

train <- subset(data_1001, date_sick_year < 2013)
pred <- subset(data_1001, date_sick_year >  2013 & date_sick_year < 2015 )

par(mfrow=c(1,1))
plot(data_1001$time, data_1001$count, type="l")
points(train$p,type="l", col="red")
points(pred$time, pred$p,type="l", col="blue")
abline(h=10, col = "gray10")

#for training data

sqrt(mean((train$count-train$p)^2,na.rm=T))/sqrt(mean((training$count)^2,na.rm=T))

#for validation data 2011-2013

sqrt(mean((pred$count-pred$p)^2,na.rm=T))/sqrt(mean((pred$count)^2, na.rm=T))

```

## Prediction using the entire data with GAM

```{r entireDataPrediction, echo= FALSE, results= "hide" }



```







## Notes to myself:

- I need to create the lagged values for each district.
- Separate the data into training and testing datasets. Keep the year variable flexible, so that I can experiment with year variables
- Row bind all the training datasets i.e. the data from 50 BKK districts
- Row bind all the data from the testing datasets similar to above.
- make tests.








